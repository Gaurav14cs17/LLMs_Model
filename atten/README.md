# ğŸ“– Attention Mechanisms: A Visual Guide

> *"Attention is All You Need"* â€” The paper that revolutionized deep learning

<p align="center">
  <img src="./images/attention-overview.svg" alt="Attention Mechanism Overview" width="800"/>
</p>

---

## ğŸ¯ How to Read This Guide

**Read like a book â€” start from Chapter 0 and progress sequentially:**

```
Chapter 0 (Concepts) â†’ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7
```

Each chapter builds on the previous one. Don't skip!

---

## ğŸ“š Chapters

### **Chapter 0: Prerequisites (Must Read!)**

| Ch | Topic | What You'll Learn |
|----|-------|-------------------|
| **0** | [Important Concepts](./00-important-concepts/) | **Why âˆšd_k?** d_model vs d_k, Q/K/V intuition, temperature |

> âš ï¸ **Many learners skip these details and get confused later. Start here!**

### **Part I: Core Concepts**

| Ch | Topic | What You'll Learn |
|----|-------|-------------------|
| **1** | [Self-Attention](./01-self-attention/) | Foundation â€” how Q, K, V work, NÃ—N attention matrix |
| **2** | [Cross-Attention](./02-cross-attention/) | How encoder-decoder attention works (NÃ—M matrix!) |
| **3** | [Multi-Head Attention](./03-multi-head-attention/) | Why multiple heads, what each head learns |

### **Part II: Special Patterns**

| Ch | Topic | What You'll Learn |
|----|-------|-------------------|
| **4** | [Causal Attention](./04-causal-attention/) | Masking for GPT-style generation (lower triangular) |
| **5** | [Sparse Attention](./05-sparse-attention/) | Efficient patterns for long sequences |

### **Part III: Deep Dive**

| Ch | Topic | What You'll Learn |
|----|-------|-------------------|
| **6** | [Score Functions](./06-score-functions/) | Additive vs Dot-Product vs Scaled |
| **7** | [Soft vs Hard Attention](./07-soft-hard-attention/) | Weighted average vs discrete selection |

---

## ğŸ”‘ The Core Intuition

Think of attention as a **soft database lookup**:

| Component | Question | Role |
|-----------|----------|------|
| **Query (Q)** | "What am I looking for?" | The question you ask |
| **Key (K)** | "What do I contain?" | The index/label of each item |
| **Value (V)** | "What I actually provide" | The actual data content |

---

## ğŸ“ The Universal Formula

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

### Step-by-Step:

| Step | Operation | Result | Purpose |
|------|-----------|--------|---------|
| **1** | QK^T | (n Ã— m) matrix | Compute all pairwise scores |
| **2** | Ã· âˆšd_k | scaled scores | **Prevent softmax saturation** |
| **3** | softmax | attention weights | Normalize to probabilities |
| **4** | Ã— V | output | Weighted combination of values |

### âš ï¸ Why âˆšd_k? (Critical!)

| Without âˆšd_k | With âˆšd_k |
|--------------|-----------|
| Variance grows with d_k | Variance = 1 (constant) |
| Scores: [-45, +45] for d_k=512 | Scores: [-3, +3] |
| Softmax saturates â†’ one-hot | Softmax works normally |
| **Gradients vanish, training fails!** | **Stable training** |

ğŸ‘‰ [Learn more in Chapter 0](./00-important-concepts/#why-divide-by-âˆšdk)

---

## ğŸ—‚ï¸ Quick Reference: Attention Types

<p align="center">
  <img src="./images/attention-taxonomy.svg" alt="Attention Taxonomy" width="900"/>
</p>

| Type | Q Source | K,V Source | Matrix Shape | Example Use |
|------|----------|------------|--------------|-------------|
| **Self** | Same | Same | N Ã— N | BERT, GPT |
| **Cross** | Target | Source | N Ã— M | Translation |
| **Causal** | Same | Same (masked) | N Ã— N (triangular) | GPT |
| **Multi-Head** | Same | Same | h Ã— (N Ã— N) | All Transformers |

---

## ğŸ“Š Complexity at a Glance

| Pattern | Time | Space | Use Case |
|---------|------|-------|----------|
| Dense (Full) | O(nÂ²) | O(nÂ²) | Short sequences |
| Sparse | O(nÂ·w) | O(nÂ·w) | Long documents |
| Linear | O(n) | O(n) | Very long sequences |

---

## ğŸš€ Start Reading!

**Begin with Chapter 0 (Important Concepts):**

ğŸ‘‰ **[Chapter 0: Important Concepts](./00-important-concepts/)** â€” Why âˆšd_k? What is d_model? Don't skip!

Then continue to Chapter 1:

ğŸ‘‰ **[Chapter 1: Self-Attention](./01-self-attention/)** â€” The foundation of everything!

---

## ğŸ“ Structure

```
atten/
â”œâ”€â”€ README.md                    â† You are here
â”œâ”€â”€ images/
â”‚
â”œâ”€â”€ 00-important-concepts/       â† START HERE (Prerequisites!)
â”‚   â”œâ”€â”€ README.md                  Why âˆšd_k? d_model vs d_k? Q/K/V?
â”‚   â””â”€â”€ images/
â”‚
â”œâ”€â”€ 01-self-attention/           â† Then continue here
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ images/
â”‚       â””â”€â”€ *.svg (NÃ—N matrix diagrams)
â”‚
â”œâ”€â”€ 02-cross-attention/
â”œâ”€â”€ 03-multi-head-attention/
â”œâ”€â”€ 04-causal-attention/
â”œâ”€â”€ 05-sparse-attention/
â”œâ”€â”€ 06-score-functions/
â””â”€â”€ 07-soft-hard-attention/
```

---

## ğŸ“ˆ Evolution Timeline

| Year | Innovation | Impact |
|------|------------|--------|
| 2014 | Bahdanau Attention | First seq2seq attention |
| 2015 | Luong Attention | Simpler dot-product |
| 2017 | Transformer | Self-attention revolution |
| 2020 | Longformer | Sparse for long docs |
| 2022 | Flash Attention | Memory-efficient |

---

<p align="center">
  <b>ğŸ‘‰ <a href="./00-important-concepts/">Start Chapter 0: Important Concepts â†’</a></b>
  <br/>
  <a href="./00-important-concepts/">Then Chapter 0: 00-important-concepts â†’</a>
</p>

---

<p align="center">
  <i>Created with ğŸ’œ for the AI community</i>
</p>
