# ğŸ›ï¸ Fine-Tuning Techniques

<p align="center">
  <img src="../assets/svg/techniques/techniques-overview.svg" alt="Fine-Tuning Techniques" width="850"/>
</p>

---

## ğŸ“š Techniques Overview

<p align="center">
  <img src="../assets/svg/techniques/technique-selection.svg" alt="Technique Selection" width="800"/>
</p>

---

## ğŸ” Detailed Guides

| Technique | Memory | Speed | Quality | Guide |
|-----------|--------|-------|---------|-------|
| Full Fine-Tune | ğŸ”´ High | ğŸŸ¡ Medium | ğŸŸ¢ Best | [Full-FT](./Full-Fine-Tuning/) |
| LoRA | ğŸŸ¢ Low | ğŸŸ¢ Fast | ğŸŸ¢ High | [LoRA](./LoRA/) |
| QLoRA | ğŸŸ¢ Very Low | ğŸŸ¡ Medium | ğŸŸ¢ High | [QLoRA](./QLoRA/) |
| DoRA | ğŸŸ¡ Medium | ğŸŸ¢ Fast | ğŸŸ¢ Best | [DoRA](./DoRA/) |
| PPO | ğŸ”´ High | ğŸ”´ Slow | ğŸŸ¢ RLHF | [PPO](./PPO/) |
| DPO | ğŸŸ¡ Medium | ğŸŸ¢ Fast | ğŸŸ¢ RLHF | [DPO](./DPO/) |

---

## ğŸ†š PEFT Comparison

<p align="center">
  <img src="../assets/svg/techniques/peft-deep-dive.svg" alt="PEFT Deep Dive" width="800"/>
</p>

---

## ğŸ”§ LoRA Architecture

<p align="center">
  <img src="../assets/svg/techniques/lora-architecture.svg" alt="LoRA Architecture" width="700"/>
</p>

---

## âš¡ QLoRA Architecture

<p align="center">
  <img src="../assets/svg/techniques/qlora-architecture.svg" alt="QLoRA Architecture" width="700"/>
</p>

---

## ğŸ­ DoRA Architecture

<p align="center">
  <img src="../assets/svg/techniques/dora-architecture.svg" alt="DoRA Architecture" width="700"/>
</p>

---

## ğŸ¯ Alignment: PPO vs DPO

<p align="center">
  <img src="../assets/svg/techniques/ppo-vs-dpo.svg" alt="PPO vs DPO" width="800"/>
</p>

---

## ğŸ”Œ Adapter Layers

<p align="center">
  <img src="../assets/svg/techniques/adapters.svg" alt="Adapters" width="750"/>
</p>

---

## ğŸ§¬ Mixture of Experts (MoE)

<p align="center">
  <img src="../assets/svg/techniques/moe.svg" alt="Mixture of Experts" width="800"/>
</p>

---

## âš¡ ORPO - Odds Ratio Preference Optimization

<p align="center">
  <img src="../assets/svg/techniques/orpo.svg" alt="ORPO" width="800"/>
</p>

---

## âœ‚ï¸ Half Fine-Tuning

<p align="center">
  <img src="../assets/svg/techniques/half-ft.svg" alt="Half Fine-Tuning" width="750"/>
</p>

---

## ğŸ§  Lamini Memory Tuning

<p align="center">
  <img src="../assets/svg/techniques/lamini.svg" alt="Lamini Memory Tuning" width="750"/>
</p>

---

## ğŸ¥ Domain-Specific Fine-Tuning

<p align="center">
  <img src="../assets/svg/techniques/domain-specific.svg" alt="Domain Specific Fine-Tuning" width="850"/>
</p>

---

## ğŸ““ Colab Notebooks

| Notebook | Description | Link |
|----------|-------------|------|
| LoRA | Parameter-efficient tuning | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](../notebooks/02_lora_fine_tuning.ipynb) |
| QLoRA | 4-bit quantized LoRA | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](../notebooks/03_qlora_fine_tuning.ipynb) |
| DPO | Direct Preference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](../notebooks/04_dpo_training.ipynb) |

---

## ğŸ“š Reference

> [A Comprehensive Guide to Fine-Tuning Large Language Models](https://arxiv.org/html/2408.13296v1)

