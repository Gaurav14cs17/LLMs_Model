# ğŸ¯ Introduction to Large Language Models

<p align="center">
  <img src="../assets/svg/intro/llm-evolution.svg" alt="LLM Evolution" width="800"/>
</p>

---

## ğŸ“Š Evolution of Language Models

<p align="center">
  <img src="../assets/svg/intro/nlp-evolution-timeline.svg" alt="NLP Evolution Timeline" width="850"/>
</p>

---

## ğŸ—ï¸ LLM Architecture

<p align="center">
  <img src="../assets/svg/intro/transformer-architecture.svg" alt="Transformer Architecture" width="700"/>
</p>

---

## ğŸŒŸ Current Leading LLMs

<p align="center">
  <img src="../assets/svg/intro/leading-llms.svg" alt="Leading LLMs" width="800"/>
</p>

---

## ğŸ”„ How LLMs Work

<p align="center">
  <img src="../assets/svg/intro/llm-workflow.svg" alt="LLM Workflow" width="750"/>
</p>

---

## ğŸ“ˆ Model Size Comparison

| Model | Parameters | Training Data | Context Length |
|-------|------------|---------------|----------------|
| GPT-4 | ~1.8T | - | 128K |
| LLaMA 3 | 70B | 15T tokens | 8K |
| Mistral | 7B | - | 32K |
| Claude 3 | - | - | 200K |
| Gemini | - | - | 1M |

---

## ğŸ”€ Types of Fine-Tuning

<p align="center">
  <img src="../assets/svg/intro/fine-tuning-types.svg" alt="Fine-Tuning Types" width="850"/>
</p>

---

## ğŸ” RAG - Retrieval Augmented Generation

<p align="center">
  <img src="../assets/svg/intro/rag-pipeline.svg" alt="RAG Pipeline" width="850"/>
</p>

---

## ğŸ†š RAG vs Fine-Tuning

<p align="center">
  <img src="../assets/svg/intro/rag-vs-finetuning.svg" alt="RAG vs Fine-Tuning" width="850"/>
</p>

---

## ğŸ”— Next Steps

| Topic | Link |
|-------|------|
| Seven Stage Pipeline | [02-Seven-Stage-Pipeline](../02-Seven-Stage-Pipeline/) |
| Data Preparation | [03-Data-Preparation](../03-Data-Preparation/) |

---

## ğŸ“š Reference

> [A Comprehensive Guide to Fine-Tuning Large Language Models](https://arxiv.org/html/2408.13296v1)

