{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Basic LLM Fine-Tuning with Hugging Face\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Gaurav14cs17/LLMs_Model/blob/main/Fine-Tuning-LLMs-Guide/notebooks/01_basic_fine_tuning.ipynb)\n",
        "\n",
        "This notebook demonstrates basic **Supervised Fine-Tuning (SFT)** of a Large Language Model.\n",
        "\n",
        "### üìã What You'll Learn\n",
        "- Load a pre-trained model from Hugging Face\n",
        "- Prepare instruction-following dataset  \n",
        "- Fine-tune using the SFTTrainer API\n",
        "- Run inference with your fine-tuned model\n",
        "\n",
        "**‚ö†Ô∏è Requirements**: GPU with 16GB+ VRAM (T4 on free Colab works!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes trl\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "OUTPUT_DIR = \"./fine-tuned-model\"\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")\n",
        "print(f\"Dataset: {len(dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, \n",
        "    torch_dtype=torch.float16, \n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"Model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format dataset for instruction tuning\n",
        "def format_instruction(sample):\n",
        "    \"\"\"Format each sample as instruction-input-output\"\"\"\n",
        "    if sample.get(\"input\", \"\"):\n",
        "        text = f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Input:\n",
        "{sample['input']}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "    else:\n",
        "        text = f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(format_instruction)\n",
        "print(f\"Sample:\\n{dataset[0]['text'][:500]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training with SFTTrainer (recommended for instruction tuning)\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    fp16=True,\n",
        "    max_seq_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"üíæ Model saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ Test the fine-tuned model\n",
        "def generate_response(prompt, max_new_tokens=128):\n",
        "    \"\"\"Generate response from fine-tuned model\"\"\"\n",
        "    formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Test it!\n",
        "test_prompt = \"Explain what machine learning is in simple terms.\"\n",
        "print(f\"üìù Prompt: {test_prompt}\")\n",
        "print(f\"ü§ñ Response: {generate_response(test_prompt)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully fine-tuned an LLM! Next steps:\n",
        "- Try [LoRA Fine-Tuning](./02_lora_fine_tuning.ipynb) for more memory-efficient training\n",
        "- Try [QLoRA](./03_qlora_fine_tuning.ipynb) if you have limited GPU memory\n",
        "- Explore [DPO Training](./04_dpo_training.ipynb) for preference alignment\n",
        "\n",
        "üìö Reference: [A Comprehensive Guide to Fine-Tuning LLMs](https://arxiv.org/html/2408.13296v1)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
