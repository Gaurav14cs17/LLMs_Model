{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ DPO Training Guide\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Gaurav14cs17/LLMs_Model/blob/main/Fine-Tuning-LLMs-Guide/notebooks/04_dpo_training.ipynb)\n",
        "\n",
        "**Direct Preference Optimization - RLHF without Reward Models!**\n",
        "\n",
        "### üî• Why DPO?\n",
        "- **No reward model needed** (unlike PPO/RLHF)\n",
        "- **Simpler training** - just preference pairs\n",
        "- **More stable** than traditional RLHF\n",
        "- **Better alignment** with human preferences\n",
        "\n",
        "### üìä Data Format Required\n",
        "```python\n",
        "{\n",
        "    \"prompt\": \"What is the capital of France?\",\n",
        "    \"chosen\": \"Paris is the capital of France.\",  # Preferred response\n",
        "    \"rejected\": \"France is in Europe.\"            # Less preferred\n",
        "}\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è Requirements**: GPU with 16GB+ VRAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and import\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes trl\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DPO requires preference pairs:\n",
        "# - prompt: The input question\n",
        "# - chosen: The preferred response  \n",
        "# - rejected: The less preferred response\n",
        "\n",
        "# Load preference dataset\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:2000]\")\n",
        "print(f\"Dataset: {len(dataset)} samples\")\n",
        "\n",
        "# DPO Config\n",
        "dpo_config = DPOConfig(\n",
        "    beta=0.1,  # DPO temperature\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    max_steps=500,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load base model for DPO\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Reference model (frozen copy for DPO)\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Models loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare DPO dataset format\n",
        "def format_hh_rlhf(sample):\n",
        "    \"\"\"Format Anthropic HH-RLHF dataset for DPO\"\"\"\n",
        "    return {\n",
        "        \"prompt\": sample[\"chosen\"].split(\"\\n\\nAssistant:\")[0] + \"\\n\\nAssistant:\",\n",
        "        \"chosen\": sample[\"chosen\"].split(\"\\n\\nAssistant:\")[-1],\n",
        "        \"rejected\": sample[\"rejected\"].split(\"\\n\\nAssistant:\")[-1],\n",
        "    }\n",
        "\n",
        "# Apply formatting\n",
        "train_dataset = dataset.map(format_hh_rlhf)\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Sample prompt: {train_dataset[0]['prompt'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DPO Training\n",
        "from trl import DPOTrainer\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=ref_model,\n",
        "    args=dpo_config,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    beta=0.1,  # DPO temperature - lower = more aggressive preference learning\n",
        "    max_length=512,\n",
        "    max_prompt_length=256,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting DPO training...\")\n",
        "dpo_trainer.train()\n",
        "print(\"‚úÖ DPO training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DPO-trained model\n",
        "OUTPUT_DIR = \"./dpo-trained-model\"\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Test the aligned model\n",
        "def generate(prompt, max_tokens=150):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.7, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"ü§ñ Testing DPO-aligned model:\")\n",
        "test_prompt = \"Human: How can I be more productive?\\n\\nAssistant:\"\n",
        "print(generate(test_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä DPO vs Other RLHF Methods\n",
        "\n",
        "| Method | Reward Model | Complexity | Stability | Memory |\n",
        "|--------|-------------|------------|-----------|--------|\n",
        "| **DPO** | ‚ùå No | ‚≠ê Simple | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê Medium |\n",
        "| PPO | ‚úÖ Yes | ‚≠ê‚≠ê‚≠ê Complex | ‚≠ê Low | ‚≠ê‚≠ê‚≠ê High |\n",
        "| RLHF | ‚úÖ Yes | ‚≠ê‚≠ê‚≠ê Complex | ‚≠ê‚≠ê Medium | ‚≠ê‚≠ê‚≠ê High |\n",
        "| ORPO | ‚ùå No | ‚≠ê Simple | ‚≠ê‚≠ê‚≠ê High | ‚≠ê Low |\n",
        "\n",
        "## üéØ Key DPO Hyperparameters\n",
        "\n",
        "- **beta (Œ≤)**: Controls preference strength (0.1-0.5 typical)\n",
        "  - Lower = more aggressive preference learning\n",
        "  - Higher = more conservative, stays closer to reference\n",
        "\n",
        "## üìö References\n",
        "- [DPO Paper](https://arxiv.org/abs/2305.18290)\n",
        "- [A Comprehensive Guide to Fine-Tuning LLMs](https://arxiv.org/html/2408.13296v1)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
