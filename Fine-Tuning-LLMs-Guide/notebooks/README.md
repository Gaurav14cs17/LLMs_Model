# ðŸ““ Google Colab Notebooks

<p align="center">
  <img src="../assets/svg/notebooks-header.svg" alt="Notebooks" width="700"/>
</p>

---

## ðŸš€ Available Notebooks

| # | Notebook | Description | Difficulty | GPU Required |
|---|----------|-------------|------------|--------------|
| 1 | [Basic Fine-Tuning](./01_basic_fine_tuning.ipynb) | Introduction to LLM fine-tuning | â­ Beginner | 16GB+ |
| 2 | [LoRA Fine-Tuning](./02_lora_fine_tuning.ipynb) | Parameter-efficient fine-tuning | â­â­ Intermediate | 16GB |
| 3 | [QLoRA Fine-Tuning](./03_qlora_fine_tuning.ipynb) | 4-bit quantized training | â­â­ Intermediate | 6GB âœ… |
| 4 | [DPO Training](./04_dpo_training.ipynb) | Preference optimization | â­â­â­ Advanced | 16GB |

---

## ðŸŽ¯ Quick Start

### Open in Google Colab

| Notebook | Open |
|----------|------|
| Basic Fine-Tuning | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Gaurav14cs17/LLMs_Model/blob/main/Fine-Tuning-LLMs-Guide/notebooks/01_basic_fine_tuning.ipynb) |
| LoRA Fine-Tuning | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Gaurav14cs17/LLMs_Model/blob/main/Fine-Tuning-LLMs-Guide/notebooks/02_lora_fine_tuning.ipynb) |
| QLoRA Fine-Tuning | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Gaurav14cs17/LLMs_Model/blob/main/Fine-Tuning-LLMs-Guide/notebooks/03_qlora_fine_tuning.ipynb) |
| DPO Training | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Gaurav14cs17/LLMs_Model/blob/main/Fine-Tuning-LLMs-Guide/notebooks/04_dpo_training.ipynb) |

---

## ðŸ’¡ Tips for Colab

1. **Use GPU Runtime**: `Runtime` â†’ `Change runtime type` â†’ `GPU`
2. **Free Tier**: T4 GPU (16GB) - sufficient for QLoRA
3. **Colab Pro**: A100 (40GB) - better for larger models
4. **Save checkpoints**: Mount Google Drive to persist models

---

## ðŸ“š Reference

> Based on [A Comprehensive Guide to Fine-Tuning Large Language Models](https://arxiv.org/html/2408.13296v1)

